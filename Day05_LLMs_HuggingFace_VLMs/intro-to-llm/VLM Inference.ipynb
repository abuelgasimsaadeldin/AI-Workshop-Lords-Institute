{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddcec24f-8933-473b-83be-e135159cf787",
   "metadata": {},
   "source": [
    "# üåü LLM Inference Hands-On (Day 5) üöÄ\n",
    "\n",
    "Welcome to the **Day 5 Hands-On Session**!  \n",
    "In this notebook, we will explore **practical applications of Large Language Models (LLMs) and Multimodal AI** using the Hugging Face Inference API.  \n",
    "\n",
    "We‚Äôll go beyond simple text generation and try out exciting capabilities such as:\n",
    "- üñºÔ∏è **Text-to-Image Generation** ‚Äì turning words into stunning visuals  \n",
    "- üé• **Text-to-Video** ‚Äì creating short clips from prompts  \n",
    "- üéôÔ∏è **Speech Recognition (ASR)** ‚Äì converting audio to text  \n",
    "- üñºÔ∏è **Image Classification** ‚Äì detecting if content is safe or not  \n",
    "- üîÄ **Image-to-Image Editing** ‚Äì transforming one image into another  \n",
    "- üß† **Multimodal Models** ‚Äì combining text + image for richer AI interactions  \n",
    "\n",
    "üí° The goal is to give you hands-on experience with **different AI model types**, while keeping things simple and interactive.  \n",
    "All examples will use Hugging Face‚Äôs **InferenceClient**, which allows us to call powerful models with just a few lines of code.\n",
    "\n",
    "---\n",
    "\n",
    "üîê Handling API Tokens\n",
    "\n",
    "For simplicity, we will hardcode our Hugging Face API token here, though it's not recommended for production or shared notebooks. Instead, consider using environment variables or a ```.env``` file for better security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd619c-7643-4ac5-aa0d-bbc5e725403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"YOUR TOKEN HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b285a79-a274-439a-9795-bda9655bde0f",
   "metadata": {},
   "source": [
    "## Text-to-Image Generation\n",
    "\n",
    "Let‚Äôs explore how to generate images from text prompts using Inference Providers. We‚Äôll use **black-forest-labs/FLUX.1-dev**, a state-of-the-art diffusion model that produces highly detailed, photorealistic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef4da0-396e-4da6-85f5-7a3353a73dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "client = InferenceClient(api_key=HF_TOKEN)\n",
    "\n",
    "image = client.text_to_image(\n",
    "    prompt=\"Astronaut riding a horse\",\n",
    "    model=\"black-forest-labs/FLUX.1-dev\"\n",
    ")\n",
    "\n",
    "# Save the generated image\n",
    "image.save(\"generated_image.png\")\n",
    "display(Image(filename=\"generated_image.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ff86c-4ced-4c1e-b218-766d016478a0",
   "metadata": {},
   "source": [
    "## Image-to-Image Editing\n",
    "\n",
    "Edit an existing image using a model that supports image-to-image manipulation. For example, you can prompt the model to: *Turn the cat into a tiger.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a9c10d-6149-4bab-af36-0cef7c86018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "client = InferenceClient(provider=\"fal-ai\", api_key=HF_TOKEN)\n",
    "\n",
    "with open(\"cat.png\", \"rb\") as image_file:\n",
    "   input_image = image_file.read()\n",
    "\n",
    "# output is a PIL.Image object\n",
    "image = client.image_to_image(\n",
    "    input_image,\n",
    "    prompt=\"Turn the cat into a tiger.\",\n",
    "    model=\"Qwen/Qwen-Image-Edit\",\n",
    ")\n",
    "\n",
    "image.save(\"transformed_cat_to_tiger.png\")\n",
    "display(Image(filename=\"transformed_cat_to_tiger.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f23ec5-d0e2-41ae-aee2-960f00d9d161",
   "metadata": {},
   "source": [
    "## Text-to-Video\n",
    "\n",
    "Generate short video clips from text descriptions using a model that supports **Text-to-Video** synthesis. For example, you can prompt: *Create a 10-second video of a sunset over the ocean with soft waves crashing on the shore.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8219c-b956-4723-9a37-9ccd0a9fd534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from IPython.display import Video, display\n",
    "import os\n",
    "\n",
    "client = InferenceClient(provider=\"replicate\", api_key=HF_TOKEN)\n",
    "\n",
    "video = client.text_to_video(\n",
    "    prompt=\"A young man walking on the street\",\n",
    "    model=\"Wan-AI/Wan2.2-T2V-A14B\",\n",
    ")\n",
    "\n",
    "# Save the video to a file (since the response is in bytes, we write it to a file)\n",
    "with open(\"generated_video.mp4\", \"wb\") as f:\n",
    "    f.write(video)\n",
    "\n",
    "# Display the video in the notebook\n",
    "display(Video(filename=\"generated_video.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b6c24-9581-4a90-8da7-2774da61b959",
   "metadata": {},
   "source": [
    "## Image Classification ‚Äî NSFW Detection\n",
    "\n",
    "Classify images to detect whether they contain unsafe or inappropriate content. Used in Social Media Content Moderation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ac4f2-f352-430d-be2e-4a0164857f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "client = InferenceClient(provider=\"hf-inference\", api_key=HF_TOKEN)\n",
    "\n",
    "# Classifying the image for NSFW content\n",
    "output = client.image_classification(\"cat.png\", model=\"Falconsai/nsfw_image_detection\")\n",
    "\n",
    "# Extract and print the classification result\n",
    "print(f\"Label: {output['label']}\")\n",
    "print(f\"Confidence: {output['confidence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635d7a6-b4fa-4684-8e28-7851ae58477a",
   "metadata": {},
   "source": [
    "## Conversational VLM\n",
    "\n",
    "Interact with **Vision-Language Models (VLMs)** that can process both images and text in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f1644-870c-4436-8467-fd43d8f9ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(provider=\"nscale\", api_key=HF_TOKEN)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe this image in one sentence.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ef8d0-c271-4060-a26a-74489f081c8e",
   "metadata": {},
   "source": [
    "### üìå Summary\n",
    "\n",
    "In this hands-on session, we explored various practical applications of **Large Language Models (LLMs)** and **Multimodal AI** using the **Hugging Face Inference API**. Here‚Äôs a recap of what we covered:\n",
    "\n",
    "1. **Text-to-Image Generation**: Generate realistic images from text descriptions.\n",
    "2. **Image-to-Image Editing**: Transform an existing image with a new prompt (e.g., *Turn a cat into a tiger*).\n",
    "3. **Text-to-Video**: Create video clips from text prompts.\n",
    "4. **Automatic Speech Recognition (ASR)**: Transcribe spoken language into text.\n",
    "5. **Image Classification ‚Äî NSFW Detection**: Detect explicit content in images.\n",
    "6. **Conversational VLM**: Engage with vision-language models for multimodal conversations (text + image).\n",
    "7. **Translation**: Translate text between languages (e.g., Hindi to English using Helsinki model).\n",
    "\n",
    "üí° **Next Steps**: Explore more models on the Hugging Face Hub, experiment with different use cases, and try out additional AI capabilities!\n",
    "\n",
    "For more APIs, check out the official documentation for **Inference Providers**:  \n",
    "[Hugging Face Inference Providers](https://huggingface.co/docs/inference-providers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project",
   "language": "python",
   "name": "llm-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
