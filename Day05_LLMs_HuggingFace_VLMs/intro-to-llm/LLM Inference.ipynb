{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727a191a",
   "metadata": {},
   "source": [
    "# AI Text Generation Hands-On üöÄ\n",
    "\n",
    "Welcome to the **Day 5 Hands-On Session**!  \n",
    "In this notebook, we will explore **Text Generation using Large Language Models (LLMs)** with Hugging Face.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Part 1: LLM Inference using Hugging Face `pipeline`\n",
    "\n",
    "This is the easiest way to try out LLM models locally, and we will be testing out different models.  \n",
    "No account or API key required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fd153-94e2-4838-8d67-56b2a1cf88a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Hugging Face transformers if not installed\n",
    "!pip install transformers hf_xet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f550b-4f89-4f63-83af-034df9c329ff",
   "metadata": {},
   "source": [
    "### üîß Text Generation Model\n",
    "\n",
    "We'll use Hugging Face to load a small **GPT-2** model and generate text from a custom prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19847c7-14d1-4afa-8aa7-f8c5a5d86414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for text generation\n",
    "from transformers import pipeline\n",
    "\n",
    "# Let's initialize the text generation pipeline using Hugging Face's GPT-2 model (or any other model).\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# You can test the text generation by providing an initial prompt.\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Now, let's generate text by feeding the model the prompt\n",
    "generated_text = generator(prompt, max_new_tokens=50, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "\n",
    "# Output the generated text\n",
    "generated_text[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488964dd-e6e1-4549-9c5f-7ecd5477a194",
   "metadata": {},
   "source": [
    "### ‚ùì Question Answering Model\n",
    "\n",
    "We'll use Hugging Face to load a lightweight **DistilBERT** model and answer questions based on a given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70b63d-2a92-4aa6-9d55-2446ac93211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the QA pipeline with a lightweight model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "# Context (the passage the model will search in)\n",
    "context = \"\"\"\n",
    "The International Islamic University Malaysia (IIUM) was established in 1983.\n",
    "It offers programs in Islamic studies, engineering, medicine, law, and more.\n",
    "The university is located in Gombak, Selangor, Malaysia.\n",
    "\"\"\"\n",
    "\n",
    "# Question\n",
    "question = \"What programs does IIUM offer?\"\n",
    "\n",
    "# Get answer\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22019c09-a917-4edd-a38d-87432c373605",
   "metadata": {},
   "source": [
    "### üìù Text Summarization Model\n",
    "\n",
    "We'll use a **DistilBART** model from Hugging Face to summarize a longer piece of text into a concise overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4caa19-aa7b-42ee-baeb-8a77f385cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "# Long text to summarize\n",
    "text = \"\"\"\n",
    "Artificial Intelligence (AI) is transforming industries across the world.\n",
    "It is being applied in healthcare to assist doctors with diagnosis,\n",
    "in finance to detect fraud and optimize trading, and in transportation\n",
    "to enable autonomous vehicles. Education also benefits from AI with\n",
    "personalized learning experiences and automated grading systems.\n",
    "However, the widespread adoption of AI raises concerns about privacy,\n",
    "job displacement, and ethical responsibility in decision-making.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "summary = summarizer(text, max_length=60, min_length=25, do_sample=False)\n",
    "\n",
    "print(\"Summary:\", summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e1d23f-5ef4-444a-b596-09f2c27a990d",
   "metadata": {},
   "source": [
    "### üåê Translation Model\n",
    "\n",
    "We'll use a **Helsinki-NLP** model to translate text from Hindi to English using Hugging Face's translation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19610b70-dd1b-405e-9012-6a8a21ca5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-hi-en\")\n",
    "\n",
    "# Example Hindi text\n",
    "text = \"‡§≠‡§æ‡§∞‡§§ ‡§è‡§ï ‡§¨‡§π‡•Å‡§§ ‡§¨‡§°‡§º‡§æ ‡§¶‡•á‡§∂ ‡§π‡•à ‡§î‡§∞ ‡§Ø‡§π‡§æ‡§Å ‡§ï‡§à ‡§≠‡§æ‡§∑‡§æ‡§è‡§Å ‡§¨‡•ã‡§≤‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡§Ç‡•§\"\n",
    "\n",
    "# Translate\n",
    "translation = translator(text, max_length=100)\n",
    "print(\"Translation:\", translation[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a65d99-1e94-4179-92fe-06ed2e3caf79",
   "metadata": {},
   "source": [
    "### üí¨ Sentiment Analysis Model\n",
    "\n",
    "We'll use Hugging Face to analyze the sentiment of a given text (whether it's **positive** or **negative**) using the default **DistilBERT** model fine-tuned on SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c8b2e-6e53-4269-b17e-6ac685843cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"I absolutely loved the movie! It was fantastic.\",\n",
    "    \"I'm really disappointed with the service.\"\n",
    "]\n",
    "\n",
    "# Analyze sentiment\n",
    "for text in texts:\n",
    "    result = sentiment_analyzer(text)[0]\n",
    "    print(f\"Text: {text}\\nSentiment: {result['label']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceeebeb-ac2b-41fb-afda-6e81a91665e4",
   "metadata": {},
   "source": [
    "### üîé Additional NLP Tasks to Explore\n",
    "\n",
    "- **Text Classification:** Categorize texts by topic, emotion, spam detection, etc.  \n",
    "- **Named Entity Recognition (NER):** Identify entities like names, locations, and dates within text.  \n",
    "- **Fill-Mask:** Predict missing words in a sentence, useful for understanding context.  \n",
    "- **Zero-Shot Classification:** Classify text into custom labels without needing task-specific training.  \n",
    "\n",
    "Feel free to explore these tasks using Hugging Face pipelines to deepen your understanding of NLP!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd33bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Part 2: Text Generation using Hugging Face Inference API\n",
    "\n",
    "This method allows us to try more powerful models hosted on Hugging Face Hub.  \n",
    "‚ö†Ô∏è You will need a free Hugging Face account and API key.\n",
    "\n",
    "1. Create an account at [huggingface.co](https://huggingface.co)\n",
    "2. Go to **Settings ‚Üí Access Tokens**\n",
    "3. Copy your token and paste it in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02399941-8992-4fcd-9262-44bf3900703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"YOUR TOKEN HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9efffab-362f-475e-9c22-444b4b3421d9",
   "metadata": {},
   "source": [
    "### ü§ñ Text Generation with Meta LLaMA (8B)\n",
    "\n",
    "Using Hugging Face‚Äôs Inference API, we send a user message to the **LLaMA 3.1 Instruct** model and get a conversational response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87575a2-cf82-465b-9bfc-2486c5bf7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "client = InferenceClient(token=HF_TOKEN)\n",
    "\n",
    "completion = client.chat_completion(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Explain the concept of quantum computing to a 10-year-old, then provide a simple Python example\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# print result\n",
    "result = completion.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ef34d-797b-4483-9c8b-e7438358df1d",
   "metadata": {},
   "source": [
    "### üîç Zero-Shot Classification\n",
    "\n",
    "Classify text into custom labels using Hugging Face‚Äôs **BART-large MNLI** model via the Inference API (no training needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3e1f2-7299-4d94-ac3a-273fbb5f0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "client = InferenceClient(token=HF_TOKEN)\n",
    "\n",
    "text = \"The new smartphone has an amazing camera and battery life.\"\n",
    "candidate_labels = [\"sports\", \"technology\", \"food\", \"health\"]\n",
    "\n",
    "response = client.zero_shot_classification(\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    text=text,\n",
    "    candidate_labels=candidate_labels\n",
    ")\n",
    "\n",
    "# Get the label with the highest score\n",
    "top_prediction = max(response, key=lambda x: x.score).label\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Predictions:\", top_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55145421-d36e-46ba-bada-08ff1a25e5fb",
   "metadata": {},
   "source": [
    "### üíª Code Generation\n",
    "\n",
    "Use Hugging Face‚Äôs **CodeGen** model to generate Python code from a text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6493ee20-6801-45b4-875b-a56e506dbb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "client = InferenceClient(model=\"mistralai/Mistral-7B-Instruct-v0.2\", token=HF_TOKEN)\n",
    "\n",
    "prompt = \"Write a short Python function to check if a number is prime.\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596bb0e-1ecf-44a1-a49e-4a176ecc281c",
   "metadata": {},
   "source": [
    "### üó£Ô∏è Task: Speech-to-Text with Hugging Face Inference API\n",
    "\n",
    "Your task is to write Python code that:\n",
    "\n",
    "- Loads the audio file named `sample1.flac` from the root directory\n",
    "- Uses Hugging Face‚Äôs Inference API to transcribe the audio into text\n",
    "- Prints the transcription result\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Use the method `automatic_speech_recognition` model from \n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7f616-ba4e-47aa-9e43-a571240a1a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e632891",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Part 3: Building a Simple GUI with Tkinter\n",
    "\n",
    "For a nicer user experience, let‚Äôs create a **Chatbot** using **Tkinter**.\n",
    "\n",
    "You can find the code for the app inside the Python file:\n",
    "\n",
    "```\n",
    "chat-with-AI/chat_llama.py\n",
    "```\n",
    "\n",
    "To run the app, execute the Python script:\n",
    "\n",
    "```\n",
    "python chat-with-AI/chat_llama.py\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deac5e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Summary\n",
    "- **Pipeline method**: Quick and easy, no API needed.  \n",
    "- **Inference API**: Access to more powerful models, requires a Hugging Face token.  \n",
    "- **Tkinter App**: Turns text generation into an interactive experience.\n",
    "\n",
    "üëâ Try modifying the prompts and see how the model responds!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project",
   "language": "python",
   "name": "llm-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
