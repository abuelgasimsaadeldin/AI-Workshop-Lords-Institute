{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727a191a",
   "metadata": {},
   "source": [
    "# AI Text Generation Hands-On üöÄ\n",
    "\n",
    "Welcome to the **Day 5 Hands-On Session**!  \n",
    "In this notebook, we will explore **Text Generation using Large Language Models (LLMs)** with Hugging Face.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Part 1: Text Generation with Hugging Face `pipeline`\n",
    "\n",
    "This is the easiest way to try out text generation.  \n",
    "No account or API key required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fd153-94e2-4838-8d67-56b2a1cf88a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Hugging Face transformers if not installed\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19847c7-14d1-4afa-8aa7-f8c5a5d86414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for text generation\n",
    "from transformers import pipeline\n",
    "\n",
    "# Let's initialize the text generation pipeline using Hugging Face's GPT-2 model (or any other model).\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# You can test the text generation by providing an initial prompt.\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Now, let's generate text by feeding the model the prompt\n",
    "generated_text = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Output the generated text\n",
    "generated_text[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd33bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Part 2: Text Generation using Hugging Face Inference API\n",
    "\n",
    "This method allows us to try more powerful models hosted on Hugging Face Hub.  \n",
    "‚ö†Ô∏è You will need a free Hugging Face account and API key.\n",
    "\n",
    "1. Create an account at [huggingface.co](https://huggingface.co)\n",
    "2. Go to **Settings ‚Üí Access Tokens**\n",
    "3. Copy your token and paste it in the code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02399941-8992-4fcd-9262-44bf3900703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"YOUR TOKEN HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd26900-f26c-4f64-b45a-b77fe679a91e",
   "metadata": {},
   "source": [
    "## Text Generation with DeepMind DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d0820-7185-4d9e-9135-96e438fab880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "client = InferenceClient(token=HF_TOKEN)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-V3-0324\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's your name?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# print result\n",
    "result = completion.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9efffab-362f-475e-9c22-444b4b3421d9",
   "metadata": {},
   "source": [
    "## Text Generation with Meta LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87575a2-cf82-465b-9bfc-2486c5bf7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "client = InferenceClient(token=HF_TOKEN)\n",
    "\n",
    "completion = client.chat_completion(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Can you please let us know more details about your\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# print result\n",
    "result = completion.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e632891",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Part 3: Building a Simple GUI with Tkinter\n",
    "\n",
    "For a nicer user experience, let‚Äôs create a **Text Generator App** using **Tkinter**.\n",
    "\n",
    "You can find the code for the app inside the Python file:\n",
    "\n",
    "```\n",
    "user-interface/chat_gui.py\n",
    "```\n",
    "\n",
    "To run the app, execute the Python script:\n",
    "\n",
    "```\n",
    "python user-interface/chat_gui.py\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deac5e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Summary\n",
    "- **Pipeline method**: Quick and easy, no API needed.  \n",
    "- **Inference API**: Access to more powerful models, requires a Hugging Face token.  \n",
    "- **Tkinter App**: Turns text generation into an interactive experience.\n",
    "\n",
    "üëâ Try modifying the prompts and see how the model responds!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project",
   "language": "python",
   "name": "llm-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
