# Day 5 – Large Language Models (LLMs) & Vision-Language Models (VLMs)

Welcome to Day 5 of the workshop!  
Today we move from **computer vision** to the world of **Large Language Models (LLMs)** and **multimodal AI**.  
We’ll explore how models can generate, understand, and interact across **text, images, audio, and video**.

---

## 📌 Learning Objectives
By the end of Day 5, you will be able to:
- Understand what LLMs are and how they are used in real-world applications.  
- Use Hugging Face pipelines and APIs for text generation and multimodal tasks.  
- Build a **simple GUI** for interacting with an LLM.  
- Explore **multimodal AI** capabilities: text-to-image, text-to-video, speech, and more.  
- Get hands-on with **Vision-Language Models (VLMs)** that combine text + image inputs.  

---

## 🧑‍💻 Hands-On Projects

### **1. Introduction to LLMs with Hugging Face**
- **Text Generation**  
  - Use Hugging Face pipelines for text generation.  
  - Try Hugging Face Inference API for hosted models.  

- **Simple GUI with Tkinter**  
  - Build a basic interface for generating text using an LLM.  

- **Exploring Beyond Text (Multimodal AI)**  
  - 🖼️ Text-to-Image generation (Diffusion Models)  
  - 🎥 Text-to-Video generation  
  - 🎙️ Automatic Speech Recognition (Whisper)  
  - 🖼️ Image Classification (e.g., NSFW detection)  
  - 🔀 Image-to-Image transformation (image editing)  
  - 🧠 Multimodal tasks (text + image prompts)  

---

### **2. Vision-Language Models (VLMs)**
- Introduction to VLMs and their applications.  
- Hands-on: Try multimodal models that accept both **text and images as input**.  
- Example tasks:  
  - Image captioning  
  - Visual question answering  
  - Zero-shot classification with text + image  
  